{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import Analysis_new as analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [20, 30, 40, 50]\n",
    "n_list= [500, 1000, 2000, 3000, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current p is 20 and current estimator is OLS\n",
      "500\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "Current p is 20 and current estimator is T-Learner\n",
      "500\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "Current p is 20 and current estimator is HRF\n",
      "500\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "{'HRF': {'500': 0.188, '1000': 0.252, '2000': 0.321, '3000': 0.244, '4000': 0.2525}}\n",
      "Current p is 20 and current estimator is CF DML\n",
      "500\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "{'HRF': {'500': 0.188, '1000': 0.252, '2000': 0.321, '3000': 0.244, '4000': 0.2525}, 'CF DML': {'500': 0.752, '1000': 0.746, '2000': 0.721, '3000': 0.78, '4000': 0.8415}}\n",
      "Current p is 20 and current estimator is GRF\n",
      "500\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [10:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     mse_simulation \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mmse_ci_analysis(p\u001b[38;5;241m=\u001b[39mp, mean_correlation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, n_list\u001b[38;5;241m=\u001b[39mn_list, estimator\u001b[38;5;241m=\u001b[39mestimator, function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquadratic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m     mse_simulation, coverage_dict \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_ci_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_correlation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     estimator_dict_coverage[key_est] \u001b[38;5;241m=\u001b[39m coverage_dict\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(estimator_dict_coverage)\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\Analysis_new.py:89\u001b[0m, in \u001b[0;36mmse_ci_analysis\u001b[1;34m(p, mean_correlation, n_list, estimator, function)\u001b[0m\n\u001b[0;32m     86\u001b[0m coverage_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_list:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(n)\n\u001b[0;32m     91\u001b[0m     li_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     92\u001b[0m     li_test \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\Methods_new.py:165\u001b[0m, in \u001b[0;36mGRF_estimator\u001b[1;34m(Y_train, T_train, X_train, X_test, true_cate_train, true_cate_test)\u001b[0m\n\u001b[0;32m    160\u001b[0m true_cate_train \u001b[38;5;241m=\u001b[39m true_cate_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m    163\u001b[0m est_grf \u001b[38;5;241m=\u001b[39m CausalForest(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[43mest_grf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m lb, ub \u001b[38;5;241m=\u001b[39m est_grf\u001b[38;5;241m.\u001b[39mpredict_interval(X_test)\n\u001b[0;32m    168\u001b[0m ci_bounds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((lb, ub, true_cate_test))\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\.conda\\Lib\\site-packages\\econml\\grf\\classes.py:392\u001b[0m, in \u001b[0;36mCausalForest.fit\u001b[1;34m(self, X, T, y, sample_weight)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, T, y, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    Build a causal forest of trees from the training set (X, T, y).\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    self : object\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\.conda\\Lib\\site-packages\\econml\\grf\\_base_grf.py:386\u001b[0m, in \u001b[0;36mBaseGRF.fit\u001b[1;34m(self, X, T, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m     s_inds \u001b[38;5;241m=\u001b[39m [subsample_random_state\u001b[38;5;241m.\u001b[39mchoice(n_samples, n_samples_subsample, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    378\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)]\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreading\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myaug\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_y_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_relevant_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_inds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\.conda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joaov\\OneDrive\\Área de Trabalho\\Causal ML Literature\\Data Set Generation\\Heter_effects\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p_dict_mse= {}\n",
    "p_dict_coverage = {}\n",
    "estimator_list = ['OLS', 'T-Learner', 'HRF', 'CF DML', 'GRF']\n",
    "\n",
    "for p in tqdm(p_list):\n",
    "    \n",
    "    estimator_dict_mse = {}\n",
    "    estimator_dict_coverage = {}\n",
    "\n",
    "    for estimator in estimator_list:\n",
    "            print(f'Current p is {p} and current estimator is {estimator}')\n",
    "            key_est = f'{estimator}'\n",
    "\n",
    "            if estimator=='OLS' or estimator == 'T-Learner':\n",
    "                mse_simulation = analysis.mse_ci_analysis(p=p, mean_correlation=0.1, n_list=n_list, estimator=estimator, function='quadratic')\n",
    "\n",
    "            else:\n",
    "                mse_simulation, coverage_dict = analysis.mse_ci_analysis(p=p, mean_correlation=0.1, n_list=n_list, estimator=estimator)\n",
    "                estimator_dict_coverage[key_est] = coverage_dict\n",
    "                print(estimator_dict_coverage)\n",
    "\n",
    "            estimator_dict_mse[key_est] = mse_simulation\n",
    "\n",
    "    key_p = f'{p}'\n",
    "    p_dict_mse[key_p] = estimator_dict_mse\n",
    "    p_dict_coverage[key_p] = estimator_dict_coverage\n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for p_key, est_dict in p_dict_coverage.items():\n",
    "    for est_key, n_dict in est_dict.items():\n",
    "        for n_key, coverage_rate in n_dict.items():\n",
    "            rows.append({'p': p_key, 'est': est_key, 'n': n_key, 'coverage_rates': coverage_rate})\n",
    "\n",
    "# Creating the DataFrame\n",
    "coverage_df = pd.DataFrame(rows, columns=['p', 'est', 'n', 'coverage_rates'])\n",
    "            \n",
    "coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_20_features_linear = p_dict_mse['20']\n",
    "mse_30_features_linear = p_dict_mse['30']\n",
    "mse_40_features_linear = p_dict_mse['40']\n",
    "mse_50_features_linear = p_dict_mse['50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse_analysis_test(mse_list: list, p_list: list):\n",
    "      \n",
    "    plt.style.use('seaborn-v0_8')\n",
    "\n",
    "    mse_list = enumerate(mse_list)\n",
    "    p_list = enumerate(p_list)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    gs = fig.add_gridspec(nrows=2, ncols=2, hspace=0.2)\n",
    "    axs = gs.subplots(sharex=True, sharey=True)\n",
    "    \n",
    "    for j in range(0,2): \n",
    "        for i in range(0, 2):\n",
    "\n",
    "            colors = enumerate(['red', 'green', 'blue', 'yellow'])\n",
    "            mse_dict = next(mse_list)[1]\n",
    "            \n",
    "            for est, mse_df in mse_dict.items():\n",
    "                color = next(colors)\n",
    "                \n",
    "                axs[j, i].scatter(mse_df['n'], mse_df['MSE Test'], s=20, marker='o', color = color[1], label=est)\n",
    "\n",
    "            p = next(p_list)[1]\n",
    "            axs[j, i].set_title(f'p $=$ {p}')\n",
    "\n",
    "\n",
    "        if j == 0 and i==1:\n",
    "            axs[j, i].legend()\n",
    "        else:\n",
    "            continue\n",
    "    # Create a single legend for the entire figure\n",
    "\n",
    "    #fig.suptitle('Influence of Individual Feature Values on the CATE Function', y=0.96)\n",
    "    fig.text(0.5, 0.04, 'No. of Observations', ha='center')\n",
    "    fig.text(0.04, 0.5, 'MSE Test', va='center', rotation='vertical')\n",
    "\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    #for ax in axs.flat:\n",
    "     #   ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_list = [mse_20_features_linear, mse_30_features_linear, mse_40_features_linear, mse_50_features_linear]\n",
    "plot_mse_analysis_test(mse_list, p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mse_20_features_low.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(mse_20_features_linear, pickle_file)\n",
    "\n",
    "with open('mse_30_features_low.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(mse_30_features_linear, pickle_file)\n",
    "\n",
    "with open('mse_40_features_low.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(mse_40_features_linear, pickle_file)\n",
    "\n",
    "with open('mse_50_features_low.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(mse_50_features_linear, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
